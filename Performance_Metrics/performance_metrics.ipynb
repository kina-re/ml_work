{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(x,n):\n",
    "    \"\"\"\n",
    "    Creates 0 or 1 class labels for probability values based on threshold\n",
    "    :param n: threshold value 0.5\n",
    "    :param x: Probability value\n",
    "    :return: 0 if value is less than 0.5 , else 1\n",
    "    \"\"\"\n",
    "    x = np.where(x < n, 0, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "-> Confusion Matrix provides insight about which classes are predicted correctly and which are predicted incorrectly and what      type of error is occured.\n",
    "\n",
    "-> It categorizes prediction into positive predictions and negative predictions,Hence it is possible to focus on one class through precision and recall and that is why it is suitable for imbalanced classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\n",
    "def confusion_matrix(true, pred):  \n",
    "    \"\"\"\n",
    "    Calculates values of confusion matrix\n",
    "    :param true: true values from true value column\n",
    "    :param pred: predicted values from proba column\n",
    "    :return: True negative, True Positive, False Positive and False negative\n",
    "    \"\"\"\n",
    "    tp = np.sum(np.logical_and(true == 1, pred == 1))\n",
    "    tn = np.sum(np.logical_and(true == 0, pred == 0))\n",
    "    fp = np.sum(np.logical_and(true == 0, pred == 1))\n",
    "    fn = np.sum(np.logical_and(true == 1, pred == 0))\n",
    "    \n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "-> Precision is number of positive predictions that are actually from the positive class.\n",
    "\n",
    "-> Precision doesn't consider false negatives, so we can not figure out how many actual positive class predictions are part of negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,fp):\n",
    "    \"\"\"\n",
    "    Calculates Precision\n",
    "    :param tp: true positive values from confusion matrix\n",
    "    :param fp: false positive values from confusion matrix\n",
    "    :return: precision in percentage\n",
    "    \"\"\"\n",
    "    return tp/float(tp+fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "-> Recall is about how well the positive class has been predicted.\n",
    "\n",
    "-> Recall considers false negatives and so it provides information about predictions that could have been positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(TP,FN):\n",
    "    \"\"\"\n",
    "    Calculates Recall\n",
    "    :param TP: True positive from performance matrix\n",
    "    :param FN: False negative from performance matrix\n",
    "    :return: percentage of recall values\n",
    "    \"\"\"\n",
    "    return TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "\n",
    "-> Accuracy score is not suitable for imbalanced classification, As it doesn't consider correctly classified points  and incorrectly classified points from different classes, rather it considers only true positiva nad true negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(tp,tn,fp,fn):\n",
    "    \"\"\"\n",
    "    Calculates accuracy score\n",
    "    :param Tp: True Positive values from performance matrix\n",
    "    :param Tn: True Negative values from performance matrix\n",
    "    :param Fp: False Positive values from performance matrix\n",
    "    :param Fn: False negative values from performance matrix\n",
    "    :return: Accuracy score as a float value\n",
    "    \"\"\"\n",
    "    \n",
    "    res = (tp+tn)/(tp+fp+fn+tn)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate\n",
    "\n",
    "-> TPR is same as recall and it tells number all correct positive predictions made out of all correct predicitons that could have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPR(TP,FN):\n",
    "    \"\"\"\n",
    "    Calculates True positive rate\n",
    "    :param TP: True Positive from confusion matrix\n",
    "    :param FN: False negative from confusion matrix\n",
    "    :return: True Positive Rate as a float value\n",
    "    \"\"\"\n",
    "    return (TP)/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate\n",
    "\n",
    "-> False positive rate is number of actual negative predicitons predicted as positive from all negative predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPR(FP, TN):\n",
    "    \"\"\"\n",
    "    Calculates false positive rate\n",
    "    :param FP: False Positive from performance matrix\n",
    "    :param TN: True negative from performance matrix\n",
    "    :return: false positive rate as a float value\n",
    "    \"\"\"\n",
    "    return (FP)/(FP+TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "-> Precision gives more weightage to false positve, Where as recall gives more weightage to false negatives.\n",
    "\n",
    "-> F1 score is geomatric mean of precision and recall and hence it balances both the precision and the recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(precision,recall):\n",
    "    \"\"\"\n",
    "     Calculates F1 score\n",
    "    :param precision: precision value in percentage\n",
    "    :param recall: Recall value in percentage\n",
    "    :return: F1 score as a float value\n",
    "    \"\"\"\n",
    "    ans = float(2*precision*recall)/float(precision+recall)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thre_desc(x):\n",
    "    \"\"\"\n",
    "    Round up threshold values to one decimal point and arrange them in descending order\n",
    "    :param x: Probability values\n",
    "    :return: Thresholds in descending order\n",
    "    \"\"\"  \n",
    "    prob_threshold = x.to_numpy()\n",
    "    y = prob_threshold.round(decimals=1)\n",
    "    z = np.unique(y)\n",
    "    thre_asce = np.sort(z)\n",
    "    thre_desc = np.flip(thre_asce)\n",
    "    return thre_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates TPR and FPR for all thresholds\n",
    "    :param y_true: Actual class labels\n",
    "    :param y_pred: Predicted probability values\n",
    "    :return: list of TPR and FPR values across various thresholds\n",
    "    \"\"\"\n",
    "    tpr_array = []\n",
    "    fpr_array = []\n",
    "    thre_list = thre_desc(y_pred)\n",
    "    for i in thre_list:\n",
    "        m = threshold(y_pred, i)\n",
    "        tp, tn, fp, fn = confusion_matrix(y_true, m)\n",
    "        tpr = TPR(tp, fn)\n",
    "        tpr_array.append(tpr)\n",
    "        fpr = FPR(fp, tn)\n",
    "        fpr_array.append(fpr)\n",
    "    return tpr_array, fpr_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prob_val'] = df['proba'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['proba'] = threshold(df['proba'],0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP when no. of +ve points >> no. of -ve points:  10000\n",
      "TN when no. of +ve points >> no. of -ve points: 0\n",
      "FP when no. of +ve points >> no. of -ve points: 100\n",
      "FN when no. of +ve points >> no. of -ve points: 0\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix values True Positive, True Negative, False Positive, False Negative calculations\n",
    "tp, tn, fp, fn = confusion_matrix(df['y'], df['proba'])\n",
    "print('TP when no. of +ve points >> no. of -ve points: ', tp)\n",
    "print('TN when no. of +ve points >> no. of -ve points:', tn)\n",
    "print('FP when no. of +ve points >> no. of -ve points:', fp)\n",
    "print('FN when no. of +ve points >> no. of -ve points:', fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision when no. of +ve points >> no. of -ve points: 0.9900990099009901\n",
      "Recall when no. of +ve points >> no. of -ve points: 1.0\n",
      "F1 Score when no. of +ve points >> no. of -ve points: 0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "# Computes F1 score using precison and recall\n",
    "\n",
    "Precision_a = precision(tp, fp)\n",
    "print('Precision when no. of +ve points >> no. of -ve points:', Precision_a)\n",
    "\n",
    "Recall_a = recall(tp, fn)\n",
    "print('Recall when no. of +ve points >> no. of -ve points:', Recall_a)\n",
    "\n",
    "print('F1 Score when no. of +ve points >> no. of -ve points:', f1_score(Precision_a, Recall_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here precision and recall both are high so it results in high F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "source": [
    "### Compute AUC Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> To compute AUC score different thresholds are used, the model which has a good score across range of threshold has high score and it separates the classes better.\n",
    "\n",
    "-> It can be used for imbalanced classification, however in case of severly imbalanced classes it may give optimistic score and not a suitable metric, for severly imbalanced classification AUC based precision and recall is better choice than AUC based on TPR-FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.48897750000000006\n"
     ]
    }
   ],
   "source": [
    "# computes tpr and fpr values and AUC socre based on it\n",
    "tpr_array, fpr_array = tpr_fpr(df['y'], df['prob_val'])\n",
    "auc = np.trapz(tpr_array, fpr_array)\n",
    "print('AUC Score :', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here classes are severly skewed so AUC score is low for imbalanced classification, it is worse than the dumb model prediction. Here Precision and Recall based AUC score can be a better alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when no. of +ve points >> no. of -ve points :  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(tp,tn,fp,fn)\n",
    "print('Accuracy Score when no. of +ve points >> no. of -ve points : ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here high accuracy score is due to majority of true positives, hence accuracy score should not be used in imbalanced classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [],
   "source": [
    "df_b = pd.read_csv('5_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  0.0  0.281035\n",
       "1  0.0  0.465152\n",
       "2  0.0  0.352793\n",
       "3  0.0  0.157818\n",
       "4  0.0  0.276648"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b['prob_val'] = df_b['proba'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b['proba'] = threshold(df_b['proba'],0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP when  no. of +ve points << no. of -ve points:  55\n",
      "TN when  no. of +ve points << no. of -ve points: 9761\n",
      "FP when  no. of +ve points << no. of -ve points: 239\n",
      "FN when  no. of +ve points << no. of -ve points: 45\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix values True Positive, True Negative, False Positive, False Negative calculations\n",
    "tp, tn, fp, fn = confusion_matrix(df_b['y'], df_b['proba'])\n",
    "print('TP when  no. of +ve points << no. of -ve points: ', tp)\n",
    "print('TN when  no. of +ve points << no. of -ve points:', tn)\n",
    "print('FP when  no. of +ve points << no. of -ve points:', fp)\n",
    "print('FN when  no. of +ve points << no. of -ve points:', fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision when no. of +ve points << no. of -ve points: 0.1870748299319728\n",
      "Recall when no. of +ve points << no. of -ve points: 0.55\n",
      "F1 Score when no. of +ve points << no. of -ve points: 0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "# Computes F1 score using precison and recall\n",
    "\n",
    "Precision_b = precision(tp, fp)\n",
    "print('Precision when no. of +ve points << no. of -ve points:', Precision_b)\n",
    "\n",
    "Recall_b = recall(tp, fn)\n",
    "print('Recall when no. of +ve points << no. of -ve points:', Recall_b)\n",
    "\n",
    "print('F1 Score when no. of +ve points << no. of -ve points:', f1_score(Precision_b, Recall_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here precision is very low and recall is low, so balancing them results in low f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score when  no. of +ve points << no. of -ve points: 0.9276825\n"
     ]
    }
   ],
   "source": [
    "# computes tpr and fpr values and AUC socre based on it\n",
    "tpr_array, fpr_array = tpr_fpr(df_b['y'], df_b['prob_val'])\n",
    "auc = np.trapz(tpr_array, fpr_array)\n",
    "print('AUC Score when  no. of +ve points << no. of -ve points:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for imbalanced classes there is a high AUC.When no. of examples in minority class is small it may give optimistic result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when no. of +ve points << no. of -ve points :  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(tp,tn,fp,fn)\n",
    "print('Accuracy Score when no. of +ve points << no. of -ve points : ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here high accuracy is due to majority class and it shouldn't be considered to measure performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('5_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.458521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.505037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.418652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.375579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y      prob\n",
       "0  0  0.458521\n",
       "1  0  0.505037\n",
       "2  0  0.418652\n",
       "3  0  0.412057\n",
       "4  0  0.375579"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147200, 0.2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_threshold(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the best threshold (similarly to ROC curve computation) of probability\n",
    "    which gives lowest values of metric A for the given data\n",
    "    :param y_true: True class labels\n",
    "    :param y_pred: Predicted probabilities\n",
    "    :return: Returns minimum value of A for best threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # store values of a with thresholds\n",
    "    a_v = []\n",
    "\n",
    "    # arrange threshold in descending order\n",
    "    thre_list = thre_desc(y_pred)\n",
    "    for i in thre_list:\n",
    "        # apply threshold values and convert prob to either 0 or 1\n",
    "        m = threshold(y_pred, i)\n",
    "        tp, tn, fp, fn = confusion_matrix(y_true, m)\n",
    "        a = (500 * fn) + (100 * fp)\n",
    "        a_v.append((a, i))\n",
    "    return min(a_v)\n",
    "\n",
    "best_threshold(df_c['y'], df_c['prob']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the best threshold is 0.2 and minimum value of A is 147200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = pd.read_csv('5_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean square error\n",
    "    :param y_true: True class labels\n",
    "    :param y_pred: Predicted class labels\n",
    "    :return: value of mean square error\n",
    "    \"\"\"\n",
    "    tot_error = 0\n",
    "    for i in range(len(y_true)):\n",
    "        pred_error = y_pred[i] - y_true[i]\n",
    "        tot_error += (pred_error ** 2)\n",
    "    mean_sqr_error = tot_error / (len(y_true))\n",
    "    return mean_sqr_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "mse = mean_square_error(df_d['y'], df_d['pred'])\n",
    "print('Mean Square Error:', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean square error is affected by outliers.As errors are squared the outliers impact is more, hence it is not suitable for data with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mape_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Mean Absolute Percentage Error\n",
    "    :param y_true: True class Lables\n",
    "    :param y_pred: Predicted class labels\n",
    "    :return: Mean Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    tot_error = 0\n",
    "    tot_true = 0\n",
    "    for i in range(len(y_true)): \n",
    "        pred_error = abs(y_pred[i] - y_true[i])\n",
    "        tot_error += pred_error\n",
    "        tot_true += y_true[i]\n",
    "    mape = tot_error / (tot_true) \n",
    "    return mape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error: 0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "mape = mape_metric(df_d['y'], df_d['pred'])\n",
    "print('Mean Absolute Percentage Error:', mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAPE considers error as percentage of error so it does provide insight about importance of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2 Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    \"\"\"\n",
    "    Calculates Mean\n",
    "    :param x: Values for which mean is to be computed\n",
    "    :return: Mean of values\n",
    "    \"\"\"\n",
    "    return sum(x) / len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(values, mean):\n",
    "    \"\"\"\n",
    "    Calculates variance\n",
    "    :param values: List of values\n",
    "    :param mean: mean of values\n",
    "    :return: variance\n",
    "    \"\"\"\n",
    "\n",
    "    return sum([(x-mean)**2 for x in values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x,x_mean,y,y_mean): \n",
    "    \n",
    "    x_mean = mean(x)\n",
    "    y_mean = mean(y)\n",
    "    covar = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(len(x))) \n",
    "    return covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficients(true, pred): \n",
    "    x = [i for i in true] \n",
    "    y = [j for j in pred] \n",
    "    x_mean, y_mean = mean(x), mean(y) \n",
    "    b1 = covariance(x,x_mean,y,y_mean) / variance(x,x_mean) \n",
    "   \n",
    "    return b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9568483147034468\n"
     ]
    }
   ],
   "source": [
    "r_square = coefficients(df_d['y'], df_d['pred'])\n",
    "print(r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R square value provides insight about how much variance is explained by the model, however it is highly subjective measure and should not be consider without residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
